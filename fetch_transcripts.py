import os, json, subprocess, sys, time
from pathlib import Path
from tqdm import tqdm
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    TranscriptsDisabled,
    NoTranscriptFound,
    VideoUnavailable,
)

# You can change this to any YouTube channel/playlist URL.
# Default is Brandon Epstein's channel "Videos" tab.
CHANNEL_URL = os.getenv("CHANNEL_URL", "https://www.youtube.com/@BrandonEpsteinOfficial/videos").strip()

OUT_DIR = Path("output")
EP_DIR = OUT_DIR / "episodes"
OUT_DIR.mkdir(parents=True, exist_ok=True)
EP_DIR.mkdir(parents=True, exist_ok=True)

def run_yt_dlp_list(url: str) -> dict:
    """
    Use yt-dlp to list ALL videos (no API key) as JSON.
    """
    # --flat-playlist: no downloads, just metadata
    # -J: dump JSON
    # -I 1:9999: index range large enough to grab all
    cmd = [
        "yt-dlp", "--flat-playlist", "-J", "-I", "1:9999", url
    ]
    r = subprocess.run(cmd, capture_output=True, text=True)
    if r.returncode != 0:
        print(r.stderr)
        sys.exit(1)
    return json.loads(r.stdout)

def safe_name(s: str) -> str:
    keep = "".join(c for c in s if c.isalnum() or c in (" ", "-", "_"))
    return keep.strip()

def get_transcript_text(video_id: str) -> str:
    """
    Try human transcript first; fall back to auto-generated; else empty.
    """
    try:
        # Try in order of preference
        transcript = None
        listing = YouTubeTranscriptApi.list_transcripts(video_id)
        for lang in ("en", "en-US"):
            if listing.find_manually_created_transcript([lang]):
                transcript = listing.find_manually_created_transcript([lang]).fetch()
                break
        if transcript is None:
            # fall back to autogenerated English if available
            for lang in ("en", "en-US"):
                if listing.find_generated_transcript([lang]):
                    transcript = listing.find_generated_transcript([lang]).fetch()
                    break
        if not transcript:
            return ""
        text = " ".join(chunk["text"] for chunk in transcript if chunk.get("text"))
        return text.strip()
    except (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable):
        return ""
    except Exception:
        return ""

def main():
    print(f"[INFO] Listing videos from: {CHANNEL_URL}")
    data = run_yt_dlp_list(CHANNEL_URL)

    # yt-dlp returns a top-level dict. For channel pages:
    # - 'entries' contains video entries with 'id' and 'title'
    entries = data.get("entries") or []
    print(f"[INFO] Found {len(entries)} videos")

    merged_sections = []
    index_rows = []

    for entry in tqdm(entries, desc="Fetching transcripts"):
        vid = entry.get("id")
        title = (entry.get("title") or f"Video {vid}").replace("\n", " ").strip()
        url = f"https://www.youtube.com/watch?v={vid}"

        text = get_transcript_text(vid)
        section = [
            f"### {title}",
            f"URL: {url}",
        ]
        if text:
            section.append("")
            section.append(text)
        else:
            section.append("")
            section.append("(No transcript available)")
        merged_sections.append("\n".join(section) + "\n\n")

        # per-episode file
        per_path = EP_DIR / f"{safe_name(title)[:80]}_{vid}.txt"
        per_path.write_text("\n".join(section) + "\n", encoding="utf-8")

        index_rows.append({
            "title": title,
            "videoId": vid,
            "url": url,
            "has_transcript": bool(text),
        })

        # small polite delay
        time.sleep(0.1)

    # merged file
    merged_path = OUT_DIR / "BrandonEpstein_ALL_Transcripts.txt"
    merged_path.write_text("".join(merged_sections), encoding="utf-8")

    # index json
    (OUT_DIR / "index.json").write_text(json.dumps(index_rows, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"[DONE] Wrote {merged_path} and per-episode files in {EP_DIR}")

if __name__ == "__main__":
    main()
